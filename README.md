# PuppyGo

Vision language model and large language model powered embodied agent.

<div>
  <p>
    <img src="https://github.com/Charmve/PuppyGo/assets/29084184/6d6bab21-f55b-477c-950c-3f3cebb9174b" width=480>
    <img src="https://github.com/Charmve/PuppyGo/assets/29084184/2c6b0b4e-0933-496b-89be-25f971ffb5e6" width=360>
  </p>
</div>

## Hereâ€™s what I did:

- Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents
- extracts affordances and constraints from large language models and vision-language models to compose 3D value maps, which are used by motion planners to zero-shot synthesize trajectories for everyday manipulation tasks.
- combine with e2e large model trainning framework, like UniAD;


## This Package Is Sponsorware ðŸ’°ðŸ’°ðŸ’°

[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/Charmve?frequency=one-time&sponsor=Charmve) https://github.com/sponsors/Charmve?frequency=one-time&sponsor=Charmve

This repo was only available to my sponsors on GitHub Sponsors until I reached 15 sponsors.

Learn more about **Sponsorware** at [github.com/sponsorware/docs](https://github.com/sponsorware/docs) ðŸ’°.
